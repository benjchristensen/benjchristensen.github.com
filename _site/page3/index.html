<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Ben Christensen &middot; 
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class=" layout-reverse">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
          Ben Christensen
      </h1>
      <p class="lead">Software Engineer at Netflix, previously Apple</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/toc/">Posts</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

      <a class="sidebar-nav-item" href="http://twitter.com/benjchristensen">Twitter</a>
      <a class="sidebar-nav-item" href="http://github.com/benjchristensen">Github</a>
      <a class="sidebar-nav-item" href="http://github.com/ReactiveX/RxJava">&bull; RxJava</a>
      <a class="sidebar-nav-item" href="http://github.com/Netflix/Hystrix">&bull; Hystrix</a>
      <a class="sidebar-nav-item" href="http://linkedin.com/in/benjchristensen">LinkedIn</a>
    </nav>


    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/architecture/code/infrastructure/performance/production/production%20problems/2012/03/01/fault-tolerance-in-a-high-volume-distributed-system/">
        Fault Tolerance in a High Volume, Distributed System
      </a>
    </h1>

    <span class="post-date">01 Mar 2012</span>

    <p>Originally posted on the <a href="http://techblog.netflix.com/2012/02/fault-tolerance-in-high-volume.html">Netflix Tech Blog</a>:</p>

<hr>

<p>In an <a href="http://techblog.netflix.com/2011/12/making-netflix-api-more-resilient.html">earlier post</a> by <a href="http://twitter.com/schmaus">Ben Schmaus</a>, we shared the principles behind our circuit-breaker implementation. In that post, Ben discusses how the Netflix API interacts with dozens of systems in our service-oriented architecture, which makes the API inherently more vulnerable to any system failures or latencies underneath it in the stack. The rest of this post provides a more technical deep-dive into how our API and other systems isolate failure, shed load and remain resilient to failures.</p>

<p>Fault Tolerance is a Requirement, Not a Feature</p>

<p>The Netflix API receives more than 1 billion incoming calls per day which in turn fans out to several billion outgoing calls (averaging a ratio of 1:6) to dozens of underlying subsystems with peaks of over 100k dependency requests per second.
<a href="http://benjchristensen.files.wordpress.com/2012/03/dependencies1.png"><img src="http://benjchristensen.files.wordpress.com/2012/03/dependencies1.png?w=800" alt=""></a></p>

<p>This all occurs in the cloud across thousands of EC2 instances.</p>

<p>Intermittent failure is guaranteed with this many variables, even if every dependency itself has excellent availability and uptime.</p>

<p>Without taking steps to ensure fault tolerance, 30 dependencies each with 99.99% uptime would result in 2+ hours downtime/month (99.99%30 = 99.7% uptime = 2+ hours in a month).</p>

<p>When a single API dependency fails at high volume with increased latency (causing blocked request threads) it can rapidly (seconds or sub-second) saturate all available Tomcat (or other container such as Jetty) request threads and take down the entire API.</p>

<p><a href="http://benjchristensen.files.wordpress.com/2012/03/dependencies3.png"><img src="http://benjchristensen.files.wordpress.com/2012/03/dependencies3.png?w=800" alt=""></a></p>

<p>Thus, it is a requirement of high volume, high availability applications to build fault tolerance into their architecture and not expect infrastructure to solve it for them.</p>

<p>Netflix DependencyCommand Implementation</p>

<p>The service-oriented architecture at Netflix allows each team freedom to choose the best transport protocols and formats (XML, JSON, Thrift, Protocol Buffers, etc) for their needs so these approaches may vary across services.</p>

<p>In most cases the team providing a service also distributes a Java client library.</p>

<p>Because of this, applications such as API in effect treat the underlying dependencies as 3rd party client libraries whose implementations are &quot;black boxes&quot;. This in turn affects how fault tolerance is achieved.</p>

<p>In light of the above architectural considerations we chose to implement a solution that uses a combination of fault tolerance approaches:</p>

<ul>
<li><p>network timeouts and retries</p></li>
<li><p>separate threads on per-dependency thread pools</p></li>
<li><p>semaphores (via a <a href="http://docs.oracle.com/javase/6/docs/api/java/util/concurrent/Semaphore.html#tryAcquire()">tryAcquire</a>, not a blocking call)</p></li>
<li><p>circuit breakers</p></li>
</ul>

<p>Each of these approaches to fault-tolerance has pros and cons but when combined together provide a comprehensive protective barrier between user requests and underlying dependencies.</p>

<p><a href="http://benjchristensen.files.wordpress.com/2012/03/faulttolerancetypes.png"><img src="http://benjchristensen.files.wordpress.com/2012/03/faulttolerancetypes.png?w=800" alt=""></a></p>

<p>The Netflix DependencyCommand implementation wraps a network-bound dependency call with a preference towards executing in a separate thread and defines fallback logic which gets executed (step 8 in flow chart below) for any failure or rejection (steps 3, 4, 5a, 6b below) regardless of which type of fault tolerance (network or thread timeout, thread pool or semaphore rejection, circuit breaker) triggered it.</p>

<p><a href="http://benjchristensen.files.wordpress.com/2012/03/dependencycommands.png"><img src="http://benjchristensen.files.wordpress.com/2012/03/dependencycommands.png?w=800" alt=""></a></p>

<p>We decided that the benefits of isolating dependency calls into separate threads outweighs the drawbacks (in most cases). Also, since the API is progressively <a href="http://techblog.netflix.com/2011/02/redesigning-netflix-api.html">moving towards increased concurrency</a> it was a win-win to achieve both fault tolerance and performance gains through concurrency with the same solution. In other words, the overhead of separate threads is being turned into a positive in many use cases by leveraging the concurrency to execute calls in parallel and speed up delivery of the Netflix experience to users.</p>

<p>Thus, most dependency calls now route through a separate thread-pool as the following diagram illustrates:</p>

<p><a href="http://benjchristensen.files.wordpress.com/2012/03/dependencies4.png"><img src="http://benjchristensen.files.wordpress.com/2012/03/dependencies4.png?w=800" alt=""></a></p>

<p>If a dependency becomes latent (the worst-case type of failure for a subsystem) it can saturate all of the threads in its own thread pool, but Tomcat request threads will timeout or be rejected immediately rather than blocking.</p>

<p><a href="http://benjchristensen.files.wordpress.com/2012/03/dependencies6.png"><img src="http://benjchristensen.files.wordpress.com/2012/03/dependencies6.png?w=800" alt=""></a></p>

<p>In addition to the isolation benefits and concurrent execution of dependency calls we have also leveraged the separate threads to enable request collapsing (automatic batching) to increase overall efficiency and reduce user request latencies.</p>

<p>Semaphores are used instead of threads for dependency executions known to not perform network calls (such as those only doing in-memory cache lookups) since the overhead of a separate thread is too high for these types of operations.</p>

<p>We also use semaphores to protect against non-trusted fallbacks. Each DependencyCommand is able to define a fallback function (discussed more below) which is performed on the calling user thread and should not perform network calls. Instead of trusting that all implementations will correctly abide to this contract, it too is protected by a semaphore so that if an implementation is done that involves a network call and becomes latent, the fallback itself won&#39;t be able to take down the entire app as it will be limited in how many threads it will be able to block.</p>

<p>Despite the use of separate threads with timeouts, we continue to aggressively set timeouts and retries at the network level (through interaction with client library owners, monitoring, audits etc).</p>

<p>The timeouts at the DependencyCommand threading level are the first line of defense regardless of how the underlying dependency client is configured or behaving but the network timeouts are still important otherwise highly latent network calls could fill the dependency thread-pool indefinitely.</p>

<p>The tripping of circuits kicks in when a DependencyCommand has passed a certain threshold of error (such as 50% error rate in a 10 second period) and will then reject all requests until health checks succeed.</p>

<p>This is used primarily to release the pressure on underlying systems (i.e. shed load) when they are having issues and reduce the user request latency by failing fast (or returning a fallback) when we know it is likely to fail instead of making every user request wait for the timeout to occur.</p>

<p>How do we respond to a user request when failure occurs?</p>

<p>In each of the options described above a timeout, thread-pool or semaphore rejection, or short-circuit will result in a request not retrieving the optimal response for our customers.</p>

<p>An immediate failure (&quot;fail fast&quot;) throws an exception which causes the app to shed load until the dependency returns to health. This is preferable to requests &quot;piling up&quot; as it keeps Tomcat request threads available to serve requests from healthy dependencies and enables rapid recovery once failed dependencies recover.</p>

<p>However, there are often several preferable options for providing responses in a &quot;fallback mode&quot; to reduce impact of failure on users. Regardless of what causes a failure and how it is intercepted (timeout, rejection, short-circuited etc) the request will always pass through the fallback logic (step 8 in flow chart above) before returning to the user to give a DependencyCommand the opportunity to do something other than &quot;fail fast&quot;.</p>

<p>Some approaches to fallbacks we use are, in order of their impact on the user experience:</p>

<ul>
<li><p>Cache: Retrieve data from local or remote caches if the realtime dependency is unavailable, even if the data ends up being stale</p></li>
<li><p>Eventual Consistency: Queue writes (such as in <a href="http://aws.amazon.com/sqs/">SQS</a>) to be persisted once the dependency is available again</p></li>
<li><p>Stubbed Data: Revert to default values when personalized options can&#39;t be retrieved</p></li>
<li><p>Empty Response (&quot;Fail Silent&quot;): Return a null or empty list which UIs can then ignore</p></li>
</ul>

<p>All of this work is to maintain maximum uptime for our users while maintaining the maximum number of features for them to enjoy the richest Netflix experience possible. As a result, our goal is to have the fallbacks deliver responses as close to what the actual dependency would deliver.</p>

<p>Example Use Case</p>

<p>Following is an example of how threads, network timeouts and retries combine:</p>

<p><a href="http://benjchristensen.files.wordpress.com/2012/03/faulttoleranceexampleconfig.png"><img src="http://benjchristensen.files.wordpress.com/2012/03/faulttoleranceexampleconfig.png?w=800" alt=""></a></p>

<p>The above diagram shows an example configuration where the dependency has no reason to hit the 99.5th percentile and thus cuts it short at the network timeout layer and immediately retries with the expectation to get median latency most of the time, and accomplish this all within the 300ms thread timeout.</p>

<p>If the dependency has legitimate reasons to sometimes hit the 99.5th percentile (i.e. cache miss with lazy generation) then the network timeout will be set higher than it, such as at 325ms with 0 or 1 retries and the thread timeout set higher (350ms+).</p>

<p>The threadpool is sized at 10 to handle a burst of 99th percentile requests, but when everything is healthy this threadpool will typically only have 1 or 2 threads active at any given time to serve mostly 40ms median calls.</p>

<p>When configured correctly a timeout at the DependencyCommand layer should be rare, but the protection is there in case something other than network latency affects the time, or the combination of connect+read+retry+connect+read in a worst case scenario still exceeds the configured overall timeout.</p>

<p>The aggressiveness of configurations and tradeoffs in each direction are different for each dependency.</p>

<p>Configurations can be changed in realtime as needed as performance characteristics change or when problems are found all without risking the taking down of the entire app if problems or misconfigurations occur.</p>

<p>Conclusion</p>

<p>The approaches discussed in this post have had a dramatic effect on our ability to tolerate and be resilient to system, infrastructure and application level failures without impacting (or limiting impact to) user experience.</p>

<p>Despite the success of this new DependencyCommand resiliency system over the past 8 months, there is still a lot for us to do in improving our fault tolerance strategies and performance, especially as we continue to add functionality, devices, customers and international markets.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/infrastructure/performance/2012/01/05/webservice-performance-on-ec2-instances/">
        WebService Performance on EC2 Instances
      </a>
    </h1>

    <span class="post-date">05 Jan 2012</span>

    <p>As part of a cost/performance analysis of <a href="http://aws.amazon.com/ec2/instance-types/">Amazon EC2 instances</a> I wrote a <a href="https://github.com/benjchristensen/WSPerformanceTest">very simple Java webapp</a> to allow simple comparison benchmarking.</p>

<p>The webapp runs in Tomcat and has a <a href="https://github.com/benjchristensen/WSPerformanceTest/blob/master/src/com/wsperformancetest/ComputationalSimulationService.java">servlet</a> which returns a JSON response after looping to generate the JSON to cause CPU computation time as if the service were doing real work. It results in a lot of iteration and string creation activity – things web services do a lot of.</p>

<p>ApacheBenchmark (ab) was used as the test client to simulate traffic and capture statistics.</p>

<p>The instance types tested were <a href="https://github.com/benjchristensen/WSPerformanceTest/blob/master/testResults/2012_January5_EC2_Testing/machine_information_ec2_m2.2xlarge.txt">m2.2xlarge</a>, <a href="https://github.com/benjchristensen/WSPerformanceTest/blob/master/testResults/2012_January5_EC2_Testing/machine_information_ec2_m2.4xlarge.txt">m2.4xlarge</a>, <a href="https://github.com/benjchristensen/WSPerformanceTest/blob/master/testResults/2012_January5_EC2_Testing/machine_information_ec2_cc1.4xlarge.txt">cc1.4xlarge</a>, and <a href="https://github.com/benjchristensen/WSPerformanceTest/blob/master/testResults/2012_January5_EC2_Testing/machine_information_ec2_cc2.8xlarge.txt">cc2.8xlarge</a>.</p>

<p>The tests determined that the cc2.8xlarge instance type – though the most expensive by unit – is potentially the most cost effective type to use when serving large volume traffic involving a cluster of servers.</p>

<p>The cost to serve 1000 requests/second at a response time of approximately 18ms for each instance type is:</p>

<ul>
<li><p>m2.2xlarge: $1.98</p></li>
<li><p>m2.4xlarge: $2.48</p></li>
<li><p>cc1.4xlarge: $1.36</p></li>
<li><p><strong>cc2.8xlarge: $1.19</strong></p></li>
</ul>

<p>Of course, this test is very simplistic and ignores a wide variety of variables, but it demonstrates that the cc1 and cc2 boxes are well worth the time to evaluate for production application usage for achieving cost/performance efficiency.</p>

<p>The following graphs show the test results that were used to calculate the above costs.</p>

<p><strong>Results per Second with Increasing Concurrency</strong></p>

<p>This demonstrates how the 32 CPU cores on the cc2.8xlarge enable significantly higher throughput.</p>

<p><img src="http://benjchristensen.files.wordpress.com/2012/01/requests_per_second_with_increasing_concurrency.png?w=800" alt=""></p>

<p><strong>Time per Request (ms) with Increasing Concurrency</strong></p>

<p>This shows how the response time degrades with increasing thread counts and how the larger boxes (particularly the cc2) scale better.</p>

<p><img src="http://benjchristensen.files.wordpress.com/2012/01/time_per_request_with_increasing_concurrency.png?w=800" alt=""></p>

<p><strong>Spreadsheet</strong></p>

<p>This screenshot of the spreadsheet shows the cost calculations at each level of concurrency.</p>

<p>The hi-lighted green is considered the optimal &quot;ceiling&quot; beyond which performance degrades and throughput plateaus. This point is considered the high-water-mark that can be used to determine the number of machines in a cluster needed to serve a given amount of traffic and thus allow comparison of different machines.</p>

<p><img src="http://benjchristensen.files.wordpress.com/2012/01/ec2-performance-data.png?w=800" alt=""></p>

<p><strong>Price per 1000 Requests per Second with Increasing Concurrency</strong></p>

<p>This scatter chart shows the cost to serve 1000 requests per second at increasing levels of concurrency on each instance type.</p>

<p><img src="http://benjchristensen.files.wordpress.com/2012/01/price_with_increasing_concurrency.png?w=800" alt=""></p>

<p>Overall, the performance of the cc1 and cc2 boxes are impressive and their cost/performance appears to be far better than the m2 instances.</p>

<p>The raw results of the tests can be found on <a href="https://github.com/benjchristensen/WSPerformanceTest/tree/master/testResults/2012_January5_EC2_Testing">Github</a>.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/code/user%20interface/2011/12/16/dynamic-stacked-bar-chart-using-d3-js/">
        Dynamic Stacked Bar Chart Using d3.js
      </a>
    </h1>

    <span class="post-date">16 Dec 2011</span>

    <p>A prototype of a stacked bar chart that can dynamically add/remove bars and update the data for each bar implemented using <a href="http://mbostock.github.com/d3/">d3.js</a>.</p>

<p>It  represents data freshness (time since update) per bar using an opacity decay so a bar fades away if it doesn&#39;t receive fresh data.</p>

<p>The examples I <a href="http://mbostock.github.com/d3/ex/population.html">found</a> <a href="http://mbostock.github.com/d3/ex/stack.html">elsewhere</a> represent static data, so my model of implementation is different in that I don&#39;t use data binding or d3.layout.stack() because I couldn&#39;t figure out how to make those work with dynamic data (if someone can show me a better way, I&#39;ll gladly accept the guidance). Thus, my implementation directly adds/removes the bars and determines the bar widths and x-position itself.</p>

<p>The use case I intend to apply this prototype for is to visualize a stream of realtime data.</p>

<p>Functionality not implemented in this prototype include things such as hover and click actions to show details of the data a bar represents.</p>

<p><img src="http://benjchristensen.files.wordpress.com/2011/12/barchart.png?w=615" alt=""></p>

<p>Here are links to the code and working example:</p>

<p><a href="http://bl.ocks.org/1488375">http://bl.ocks.org/1488375</a>
<a href="https://gist.github.com/1488375"> https://gist.github.com/1488375</a></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/code/user%20interface/2011/12/13/animated-circle-using-d3-js/">
        Animated Circle Using d3.js
      </a>
    </h1>

    <span class="post-date">13 Dec 2011</span>

    <p>While working on visualizing data (application traffic) in realtime I used circles with size representing volume and color representing health.</p>

<p>Here are basic examples of circles with varying sizes and colors and animating them to dynamically change that I used as building blocks.</p>

<p><img src="http://benjchristensen.files.wordpress.com/2011/12/circles-screenshot.png?w=800" alt=""></p>

<p>Here are links to the code and working example:</p>

<p><a href="http://bl.ocks.org/1473535">http://bl.ocks.org/1473535</a>
<a href="https://gist.github.com/1473535">https://gist.github.com/1473535</a></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/code/production/2011/12/09/making-the-netflix-api-more-resilient/">
        Making the Netflix API More Resilient
      </a>
    </h1>

    <span class="post-date">09 Dec 2011</span>

    <p>A new <a href="http://techblog.netflix.com/2011/12/making-netflix-api-more-resilient.html">Netflix Tech Blog post</a> by my manager (<a href="https://twitter.com/#!/schmaus">Ben Schmaus</a>) discusses how we&#39;ve been making the Netflix API more resilient through the use of circuit breakers, bounded thread-pools and realtime decision making:</p>

<blockquote>Here are some of the key principles that informed our thinking as we set out to make the API more resilient.

> 
> 
    
>   1. A failure in a service dependency should not break the user experience for members
> 
    
>   2. The API should automatically take corrective action when one of its service dependencies fails
> 
    
>   3. The API should be able to show us what’s happening right now, in addition to what was happening 15-30 minutes ago, yesterday, last week, etc.
> 

</blockquote>

<p>A video showing the realtime monitoring dashboard is on Vimeo:</p>

<p>[vimeo http://vimeo.com/33359539]</p>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page4">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page2">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
